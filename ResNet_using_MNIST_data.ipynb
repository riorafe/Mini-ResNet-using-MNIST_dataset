{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet using MNIST_data",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riorafe/Mini-ResNet-using-MNIST_dataset/blob/master/ResNet_using_MNIST_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzFPSb0j8rJ-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "1. Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz6T5QqI8dTx",
        "colab_type": "code",
        "outputId": "5f0173a4-0ab2-4250-ab48-5dc6fac244a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBzeBZw-UTCk",
        "colab_type": "text"
      },
      "source": [
        "2. Start to code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Si_LzV82bX",
        "colab_type": "code",
        "outputId": "706d48ec-d3af-4f76-feaf-e674f86f81a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        }
      },
      "source": [
        "# ============================\n",
        "# == Made by: Rio Rafelino ===\n",
        "# Forky Engineering Specialist\n",
        "# ============================\n",
        "\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# variabel buat disesuaikan\n",
        "dataDir = '/content/drive/My Drive/MNIST_data/'\n",
        "modelDir = '/content/drive/My Drive/Model/'\n",
        "imgSize = (28,28) # ukuran MNIST dataset\n",
        "inputChannel = 1 # gambar sudah grayscale\n",
        "num_output = 10 # klasifikasi digit menjadi 10 kelas\n",
        "earlyStopping = 5\n",
        "\n",
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.x = tf.placeholder(tf.float32, shape = (None, imgSize[0], imgSize[1], inputChannel), name = 'x_placeholder')\n",
        "    self.y_true = tf.placeholder(tf.float32, shape = [None, num_output], name= 'y_true_placeholder')\n",
        "    self.y_prediction = tf.placeholder(tf.float32, shape = [None, num_output], name = 'y_prediction_placeholder')\n",
        "    self.setupCNN()\n",
        "    self.learningRate = 0.01\n",
        "    self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = self.y_true, logits = self.y_prediction)\n",
        "    self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learningRate)\n",
        "    self.train = self.optimizer.minimize(tf.reduce_mean(self.loss))\n",
        "    (self.sess, self.saver) = self.setupTF()\n",
        "\n",
        "  def setupCNN(self):\n",
        "    input_img = tf.reshape(self.x, [-1, imgSize[0], imgSize[1], inputChannel])\n",
        "    \n",
        "    kernelVals = [5, 5, 3, 3, 3]\n",
        "    featureVals = [1, 32, 64, 64, 128, 128, 256, 256]\n",
        "    strideVals = poolVals = [(2,2), (2,2), (1,1), (1,1), (1,1)]\n",
        "    numLayers = len(strideVals)\n",
        "\n",
        "    layer = input_img\n",
        "\n",
        "    for i in range (numLayers):\n",
        "      shortcut_layer = layer\n",
        "\n",
        "      initializer = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "      kernel = tf.Variable(initializer([kernelVals[i], kernelVals[i], featureVals[i], featureVals[i + 1]]), name='k1_'+str(i))\n",
        "      layer = tf.nn.conv2d(layer, kernel, padding='SAME',  strides=(1,1,1,1), name='c1_'+str(i))\n",
        "      # layer = tf.layers.batch_normalization(layer, name='b1_'+str(i))\n",
        "      layer = tf.layers.batch_normalization(layer)\n",
        "      layer = tf.nn.relu(layer, name = 'r1_'+str(i))\n",
        "\n",
        "      kernel = tf.Variable(initializer([kernelVals[i], kernelVals[i], featureVals[i+1], featureVals[i + 2]]), name='k2_'+str(i))\n",
        "      layer = tf.nn.conv2d(layer, kernel, padding='SAME',  strides=(1,1,1,1), name='c2_'+str(i))\n",
        "      # layer = tf.layers.batch_normalization(layer, name='b2_'+str(i))\n",
        "      layer = tf.layers.batch_normalization(layer)\n",
        "      layer = tf.nn.relu(layer, name = 'r2_'+str(i))\n",
        "\n",
        "      if i >= 1:\n",
        "        kernel = tf.Variable(initializer([kernelVals[i], kernelVals[i], featureVals[i+2], featureVals[i + 3]]), name='k3_'+str(i))\n",
        "        layer = tf.nn.conv2d(layer, kernel, padding='SAME',  strides=(1,1,1,1), name='c3_'+str(i))\n",
        "        # layer = tf.layers.batch_normalization(layer, name='b3_'+str(i))\n",
        "        layer = tf.layers.batch_normalization(layer)\n",
        "\n",
        "        shortcut_layer = tf.nn.conv2d(shortcut_layer, kernel, padding='SAME',  strides=(1,1,1,1), name ='cs_'+str(i))\n",
        "        # shortcut_layer = tf.layers.batch_normalization(shortcut_layer, name ='bs_'+str(i))\n",
        "        shortcut_layer = tf.layers.batch_normalization(shortcut_layer)\n",
        "\n",
        "        layer = layer + shortcut_layer\n",
        "        layer = tf.nn.relu(layer, name='r3_'+str(i))\n",
        "      else:\n",
        "        kernel = tf.Variable(initializer([kernelVals[i], kernelVals[i], featureVals[i+2], featureVals[i + 3]]), name='k3_'+str(i))\n",
        "        layer = tf.nn.conv2d(layer, kernel, padding='SAME',  strides=(1,1,1,1), name='c3_'+str(i))\n",
        "        # layer = tf.layers.batch_normalization(layer, name='b3_'+str(i))\n",
        "        layer = tf.layers.batch_normalization(layer)\n",
        "        layer = tf.nn.relu(layer, name='r3_'+str(i))\n",
        "      \n",
        "      layer = tf.nn.max_pool(layer, (1, poolVals[i][0], poolVals[i][1], 1), (1, strideVals[i][0], strideVals[i][1], 1), 'VALID', name='p_'+str(i))\n",
        "\n",
        "    output_flatten = tf.reshape(layer, [-1, 7 * 7 * 256])\n",
        "\n",
        "    # fully connected\n",
        "    weight1 = tf.Variable(tf.random_normal([12544, 1568]))\n",
        "    bias1 = tf.Variable(tf.random_normal([1568]))\n",
        "    y1 = tf.matmul(output_flatten, weight1) + bias1\n",
        "\n",
        "    weight2 = tf.Variable(tf.random_normal([1568, num_output]))\n",
        "    bias2 = tf.Variable(tf.random_normal([num_output]))\n",
        "    self.y_prediction = tf.matmul(y1, weight2) + bias2\n",
        "    \n",
        "  def setupTF(self):\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(max_to_keep=1)\n",
        "    latestSnap = tf.train.latest_checkpoint(modelDir)\n",
        "    # cek apakah ada trained model\n",
        "    if latestSnap :\n",
        "      saver.restore(sess, latestSnap)\n",
        "    else:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "    return (sess,saver);\n",
        "\n",
        "  def trainBatch(self, x_batch, y_batch, batch):\n",
        "    self.learningRate = 0.01 if batch < 5 else (0.001 if batch < 50 else 0.0001) # learning rate decay\n",
        "    self.sess.run(self.train, feed_dict={\n",
        "        self.x: x_batch,\n",
        "        self.y_true: y_batch\n",
        "    })\n",
        "  \n",
        "  def validate(self, x_batch, y_batch):\n",
        "    matches = tf.equal(tf.argmax(self.y_true, 1), tf.argmax(self.y_prediction, 1))\n",
        "    acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
        "    acc_val = self.sess.run(acc, feed_dict={\n",
        "        self.x: x_batch,\n",
        "        self.y_true: y_batch\n",
        "    })\n",
        "    return acc_val\n",
        "  \n",
        "  def save(self):\n",
        "    self.saver.save(self.sess, modelDir)\n",
        "    \n",
        "class Main:\n",
        "  def __init__(self):\n",
        "    model = Model()\n",
        "    data = self.dataLoader()\n",
        "    self.train(model, data)\n",
        "  \n",
        "  def dataLoader(self):\n",
        "    try:\n",
        "      mnist = input_data.read_data_sets('/content/drive/My Drive/MNIST_data/', one_hot=True)\n",
        "      print(\"\\nData loaded successfully\\n\")\n",
        "      return mnist\n",
        "    except:\n",
        "      print(\"\\nData failed to load\\n\")\n",
        "\n",
        "  def train(self, model, data):\n",
        "    epoch = 0\n",
        "    noImprovementSince = 0\n",
        "    bestAccVal = 0\n",
        "    \n",
        "    while True:\n",
        "      epoch += 1\n",
        "      # load dan preprocess data tiap batch\n",
        "      x_batch, y_batch = data.train.next_batch(50)\n",
        "      # reshape dari flatten balik ke bentuk image\n",
        "      x_batch = np.array([image.reshape(28, 28, 1) for image in x_batch])\n",
        "      y_batch = np.float32(y_batch)\n",
        "      model.trainBatch(x_batch, y_batch, epoch)\n",
        "      accVal = self.validate(model, data)\n",
        "      print ('Batch ke: ', epoch, 'Akurasi: ', accVal*100)\n",
        "\n",
        "      if accVal > bestAccVal:\n",
        "        bestAccVal = accVal\n",
        "        noImprovementSince = 0\n",
        "        model.save()\n",
        "        print('Model lebih baik, model disimpan..')\n",
        "      else:\n",
        "        noImprovementSince += 1\n",
        "        print('Model tidak lebih baik, tidak ada perubahan sejak: ', noImprovementSince)\n",
        "      \n",
        "      if noImprovementSince >= earlyStopping:\n",
        "        print('Training dihentikan')\n",
        "        break\n",
        "  \n",
        "  def validate(self, model, data):\n",
        "    x_batch = data.test.images\n",
        "    y_batch = data.test.labels\n",
        "    x_batch = np.array([image.reshape(28, 28, 1) for image in x_batch])\n",
        "    y_batch = np.float32(y_batch)\n",
        "    acc = model.validate(x_batch, y_batch)\n",
        "    return acc\n",
        "\n",
        "Main()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting /content/drive/My Drive/MNIST_data/train-images-idx3-ubyte.gz\n",
            "Extracting /content/drive/My Drive/MNIST_data/train-labels-idx1-ubyte.gz\n",
            "Extracting /content/drive/My Drive/MNIST_data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /content/drive/My Drive/MNIST_data/t10k-labels-idx1-ubyte.gz\n",
            "\n",
            "Data loaded successfully\n",
            "\n",
            "Batch ke:  1 Akurasi:  11.349999904632568\n",
            "Model lebih baik, model disimpan..\n",
            "Batch ke:  2 Akurasi:  10.100000351667404\n",
            "Model tidak lebih baik, tidak ada perubahan sejak:  1\n",
            "Batch ke:  3 Akurasi:  8.919999748468399\n",
            "Model tidak lebih baik, tidak ada perubahan sejak:  2\n",
            "Batch ke:  4 Akurasi:  9.82000008225441\n",
            "Model tidak lebih baik, tidak ada perubahan sejak:  3\n",
            "Batch ke:  5 Akurasi:  10.279999673366547\n",
            "Model tidak lebih baik, tidak ada perubahan sejak:  4\n",
            "Batch ke:  6 Akurasi:  9.799999743700027\n",
            "Model tidak lebih baik, tidak ada perubahan sejak:  5\n",
            "Training dihentikan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Main at 0x7f0163f4a748>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    }
  ]
}