{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet using MNIST_data",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riorafe/Mini-ResNet-using-MNIST_dataset/blob/master/ResNet_using_MNIST_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOnI9avy2W8-",
        "colab_type": "text"
      },
      "source": [
        "**Lets get started**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzFPSb0j8rJ-",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "1. Mount google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hz6T5QqI8dTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj07cwBB2UA-",
        "colab_type": "text"
      },
      "source": [
        "Before begin to code, we should **understand the dataset**\n",
        "\n",
        "Run the code below, I have prepared the demonstration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPKkZGHk9pNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# import data and set one_hot param True so the label will be encoded with one_hot encoder automatically\n",
        "mnist = input_data.read_data_sets('/content/drive/My Drive/MNIST_data/', one_hot=True)\n",
        "# we take the 50 data per batch\n",
        "x_batch, y_batch = mnist.train.next_batch(50)\n",
        "\n",
        "\n",
        "print('\\nAs we know, NIST dataset is image with 28x28 dimension, but in MNIST dataset, they got flatten')\n",
        "print('\\nShape of original x_batch (flatten): ', x_batch.shape)\n",
        "print('Data type of original x_batch: ', x_batch.dtype, '\\n')\n",
        "print(x_batch[0]) # UNCOMMENT THIS ONE IF U WANT TO SEE\n",
        "print('\\nand its already got normalized by 255, so we dont need to normalize it again\\n')\n",
        "\n",
        "\n",
        "print('\\nThen for label data,')\n",
        "print('shape of original y_batch: ', y_batch.shape)\n",
        "print('data type of original y_batch: ', y_batch.dtype, '\\n')\n",
        "print(y_batch[0])\n",
        "print('Looks like our label data are ready to be trained\\n')\n",
        "\n",
        "\n",
        "print('\\nBut i need the x_batch to be in [50, 28, 28, 1] shape to fit in my neural net')\n",
        "x_batch = np.array([image.reshape(28, 28, 1) for image in x_batch])\n",
        "print('so i do -> x_batch = np.array([image.reshape(28, 28, 1) for image in x_batch])')\n",
        "print('\\nShape of reshaped x_batch : ', x_batch.shape)\n",
        "print('Data type of reshaped x_batch: ', x_batch.dtype, '\\n')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBzeBZw-UTCk",
        "colab_type": "text"
      },
      "source": [
        "2. Start to code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0Si_LzV82bX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ============================\n",
        "# == Made by: Rio Rafelino ===\n",
        "# Forky Engineering Specialist\n",
        "# ============================\n",
        "\n",
        "import tensorflow as tf \n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# variabel buat disesuaikan\n",
        "dataDir = '/content/drive/My Drive/MNIST_data/'\n",
        "modelDir = '/content/drive/My Drive/Model/'\n",
        "imgSize = (28,28) # ukuran MNIST dataset\n",
        "inputChannel = 1 # gambar sudah grayscale\n",
        "num_output = 10 # klasifikasi digit menjadi 10 kelas\n",
        "earlyStopping = 5\n",
        "\n",
        "class Model:\n",
        "  def __init__(self):\n",
        "    self.x = tf.placeholder(tf.float32, shape = (None, imgSize[0], imgSize[1], inputChannel), name = 'x_placeholder')\n",
        "    self.y_true = tf.placeholder(tf.float32, shape = [None, num_output], name= 'y_true_placeholder')\n",
        "    self.y_prediction = tf.placeholder(tf.float32, shape = [None, num_output], name = 'y_prediction_placeholder')\n",
        "    self.setupCNN()\n",
        "    self.learningRate = 0.01\n",
        "    self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(labels = self.y_true, logits = self.y_prediction)\n",
        "    self.optimizer = tf.train.AdamOptimizer(learning_rate = self.learningRate)\n",
        "    self.train = self.optimizer.minimize(tf.reduce_mean(self.loss))\n",
        "    (self.sess, self.saver) = self.setupTF()\n",
        "\n",
        "  def setupCNN(self):\n",
        "    input_img = tf.reshape(self.x, [-1, imgSize[0], imgSize[1], inputChannel])\n",
        "    \n",
        "    kernelVals = [5, 5, 3, 3, 3]\n",
        "    featureVals = [1, 32, 64, 64, 128, 128, 256, 256]\n",
        "    strideVals = poolVals = [(2,2), (2,2), (1,1), (1,1), (1,1)]\n",
        "    numLayers = len(strideVals)\n",
        "\n",
        "    layer = input_img\n",
        "\n",
        "    for i in range (numLayers):\n",
        "      shortcut_layer = layer\n",
        "\n",
        "      initializer = tf.contrib.layers.xavier_initializer()\n",
        "\n",
        "      kernel = tf.Variable(initializer([kernelVals[i], kernelVals[i], featureVals[i], featureVals[i + 1]]), name='k1_'+str(i))\n",
        "      layer = tf.nn.conv2d(layer, kernel, padding='SAME',  strides=(1,1,1,1), name='c1_'+str(i))\n",
        "      # layer = tf.layers.batch_normalization(layer, name='b1_'+str(i))\n",
        "      layer = tf.layers.batch_normalization(layer)\n",
        "      layer = tf.nn.relu(layer, name = 'r1_'+str(i))\n",
        "\n",
        "      kernel = tf.Variable(initializer([kernelVals[i], kernelVals[i], featureVals[i+1], featureVals[i + 2]]), name='k2_'+str(i))\n",
        "      layer = tf.nn.conv2d(layer, kernel, padding='SAME',  strides=(1,1,1,1), name='c2_'+str(i))\n",
        "      # layer = tf.layers.batch_normalization(layer, name='b2_'+str(i))\n",
        "      layer = tf.layers.batch_normalization(layer)\n",
        "      layer = tf.nn.relu(layer, name = 'r2_'+str(i))\n",
        "\n",
        "      if i >= 1:\n",
        "        kernel = tf.Variable(initializer([kernelVals[i], kernelVals[i], featureVals[i+2], featureVals[i + 3]]), name='k3_'+str(i))\n",
        "        layer = tf.nn.conv2d(layer, kernel, padding='SAME',  strides=(1,1,1,1), name='c3_'+str(i))\n",
        "        # layer = tf.layers.batch_normalization(layer, name='b3_'+str(i))\n",
        "        layer = tf.layers.batch_normalization(layer)\n",
        "\n",
        "        shortcut_layer = tf.nn.conv2d(shortcut_layer, kernel, padding='SAME',  strides=(1,1,1,1), name ='cs_'+str(i))\n",
        "        # shortcut_layer = tf.layers.batch_normalization(shortcut_layer, name ='bs_'+str(i))\n",
        "        shortcut_layer = tf.layers.batch_normalization(shortcut_layer)\n",
        "\n",
        "        layer = layer + shortcut_layer\n",
        "        layer = tf.nn.relu(layer, name='r3_'+str(i))\n",
        "      else:\n",
        "        kernel = tf.Variable(initializer([kernelVals[i], kernelVals[i], featureVals[i+2], featureVals[i + 3]]), name='k3_'+str(i))\n",
        "        layer = tf.nn.conv2d(layer, kernel, padding='SAME',  strides=(1,1,1,1), name='c3_'+str(i))\n",
        "        # layer = tf.layers.batch_normalization(layer, name='b3_'+str(i))\n",
        "        layer = tf.layers.batch_normalization(layer)\n",
        "        layer = tf.nn.relu(layer, name='r3_'+str(i))\n",
        "      \n",
        "      layer = tf.nn.max_pool(layer, (1, poolVals[i][0], poolVals[i][1], 1), (1, strideVals[i][0], strideVals[i][1], 1), 'VALID', name='p_'+str(i))\n",
        "\n",
        "    output_flatten = tf.reshape(layer, [-1, 7 * 7 * 256])\n",
        "\n",
        "    # fully connected\n",
        "    weight1 = tf.Variable(tf.random_normal([12544, 1568]))\n",
        "    bias1 = tf.Variable(tf.random_normal([1568]))\n",
        "    y1 = tf.matmul(output_flatten, weight1) + bias1\n",
        "\n",
        "    weight2 = tf.Variable(tf.random_normal([1568, num_output]))\n",
        "    bias2 = tf.Variable(tf.random_normal([num_output]))\n",
        "    self.y_prediction = tf.matmul(y1, weight2) + bias2\n",
        "    \n",
        "  def setupTF(self):\n",
        "    sess = tf.Session()\n",
        "    saver = tf.train.Saver(max_to_keep=1)\n",
        "    latestSnap = tf.train.latest_checkpoint(modelDir)\n",
        "    # cek apakah ada trained model\n",
        "    if latestSnap :\n",
        "      saver.restore(sess, latestSnap)\n",
        "    else:\n",
        "      sess.run(tf.global_variables_initializer())\n",
        "    return (sess,saver);\n",
        "\n",
        "  def trainBatch(self, x_batch, y_batch, batch):\n",
        "    self.learningRate = 0.01 if batch < 5 else (0.001 if batch < 50 else 0.0001) # learning rate decay\n",
        "    self.sess.run(self.train, feed_dict={\n",
        "        self.x: x_batch,\n",
        "        self.y_true: y_batch\n",
        "    })\n",
        "  \n",
        "  def validate(self, x_batch, y_batch):\n",
        "    matches = tf.equal(tf.argmax(self.y_true, 1), tf.argmax(self.y_prediction, 1))\n",
        "    acc = tf.reduce_mean(tf.cast(matches, tf.float32))\n",
        "    acc_val = self.sess.run(acc, feed_dict={\n",
        "        self.x: x_batch,\n",
        "        self.y_true: y_batch\n",
        "    })\n",
        "    return acc_val\n",
        "  \n",
        "  def save(self):\n",
        "    self.saver.save(self.sess, modelDir)\n",
        "    \n",
        "class Main:\n",
        "  def __init__(self):\n",
        "    model = Model()\n",
        "    data = self.dataLoader()\n",
        "    self.train(model, data)\n",
        "  \n",
        "  def dataLoader(self):\n",
        "    try:\n",
        "      mnist = input_data.read_data_sets('/content/drive/My Drive/MNIST_data/', one_hot=True)\n",
        "      print(\"\\nData loaded successfully\\n\")\n",
        "      return mnist\n",
        "    except:\n",
        "      print(\"\\nData failed to load\\n\")\n",
        "\n",
        "  def train(self, model, data):\n",
        "    epoch = 0\n",
        "    noImprovementSince = 0\n",
        "    bestAccVal = 0\n",
        "    \n",
        "    while True:\n",
        "      epoch += 1\n",
        "      # load dan preprocess data tiap batch\n",
        "      x_batch, y_batch = data.train.next_batch(50)\n",
        "      # reshape dari flatten balik ke bentuk image\n",
        "      x_batch = np.array([image.reshape(28, 28, 1) for image in x_batch])\n",
        "      y_batch = np.float32(y_batch)\n",
        "      model.trainBatch(x_batch, y_batch, epoch)\n",
        "      accVal = self.validate(model, data)\n",
        "      print ('Batch ke: ', epoch, 'Akurasi: ', accVal*100)\n",
        "\n",
        "      if accVal > bestAccVal:\n",
        "        bestAccVal = accVal\n",
        "        noImprovementSince = 0\n",
        "        model.save()\n",
        "        print('Model lebih baik, model disimpan..')\n",
        "      else:\n",
        "        noImprovementSince += 1\n",
        "        print('Model tidak lebih baik, tidak ada perubahan sejak: ', noImprovementSince)\n",
        "      \n",
        "      if noImprovementSince >= earlyStopping:\n",
        "        print('Training dihentikan')\n",
        "        break\n",
        "  \n",
        "  def validate(self, model, data):\n",
        "    x_batch = data.test.images\n",
        "    y_batch = data.test.labels\n",
        "    x_batch = np.array([image.reshape(28, 28, 1) for image in x_batch])\n",
        "    y_batch = np.float32(y_batch)\n",
        "    acc = model.validate(x_batch, y_batch)\n",
        "    return acc\n",
        "\n",
        "Main()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}